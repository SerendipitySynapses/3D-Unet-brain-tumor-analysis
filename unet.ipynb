{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout,concatenate\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, Callback ,CSVLogger\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, list_IDs, input_dir, batch_size=9, dim=(128, 128), n_channels=4, n_classes=4, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.input_dir = input_dir\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        print(list_IDs_temp)\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __load_nifti_file(self, filepath):\n",
    "        scan = nib.load(filepath)\n",
    "        return scan.get_fdata()\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size * 155, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size * 155, *self.dim, self.n_classes), dtype=int)\n",
    "        with tqdm(total=self.batch_size * 155, desc='Generating batch data', unit='slice') as pbar:\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                # id_str = f\"{int(ID):03d}\"\n",
    "                folder_name = os.path.join(self.input_dir, f'BraTS20_Training_{ID}')\n",
    "                mask_name = f'BraTS20_Training_{ID}_seg'\n",
    "                npy_mask = self.__load_nifti_file(os.path.join(folder_name, mask_name + '.nii'))\n",
    "                for h in range(155):\n",
    "                    mask = tf.one_hot(npy_mask[:, :, h], self.n_classes)\n",
    "                    y[i * 155 + h] = tf.image.resize(mask, self.dim).numpy()\n",
    "                    for k, modality in enumerate(['flair', 't1', 't1ce', 't2']):\n",
    "                        channel_name = f'BraTS20_Training_{ID}_{modality}'\n",
    "                        nmpy_channel = self.__load_nifti_file(os.path.join(folder_name, channel_name + '.nii'))\n",
    "                        X[i * 155 + h, :, :, k] = cv2.resize(nmpy_channel[:, :, h], self.dim)\n",
    "                    pbar.update(1)\n",
    "\n",
    "        y[y == self.n_classes] = self.n_classes - 1\n",
    "        return X / np.max(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare IDs and data generators\n",
    "input_dir = './Dataset/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
    "train_and_test_ids = [f.name.split('_')[-1] for f in os.scandir(input_dir) if f.is_dir()]\n",
    "train_and_test_ids, val_ids = train_test_split(train_and_test_ids, test_size=0.2,random_state=42)\n",
    "train_ids, test_ids = train_test_split(train_and_test_ids, test_size=0.15, random_state=42)\n",
    "train_generator = DataGenerator(train_ids, input_dir)\n",
    "val_generator = DataGenerator(val_ids, input_dir)\n",
    "test_generator = DataGenerator(test_ids, input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, num_filters, kernel_initializer, dropout_rate=None):\n",
    "    conv = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_initializer=kernel_initializer)(inputs)\n",
    "    conv = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_initializer=kernel_initializer)(conv)\n",
    "    if dropout_rate:\n",
    "        conv = Dropout(dropout_rate)(conv)\n",
    "    return conv\n",
    "\n",
    "def up_conv_block(inputs, skip_connection, num_filters, kernel_initializer):\n",
    "    up = UpSampling2D(size=(2, 2))(inputs)\n",
    "    up = Conv2D(num_filters, 2, activation='relu', padding='same', kernel_initializer=kernel_initializer)(up)\n",
    "    merge = concatenate([skip_connection, up], axis=3)\n",
    "    conv = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_initializer=kernel_initializer)(merge)\n",
    "    conv = Conv2D(num_filters, 3, activation='relu', padding='same', kernel_initializer=kernel_initializer)(conv)\n",
    "    return conv\n",
    "\n",
    "def unet(ker_init, dropout):\n",
    "    inputs = Input((128, 128, 4))\n",
    "    # Downsampling path\n",
    "    conv1 = conv_block(inputs, 32, ker_init)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = conv_block(pool1, 64, ker_init)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = conv_block(pool2, 128, ker_init)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = conv_block(pool3, 256, ker_init)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    conv5 = conv_block(pool4, 512, ker_init, dropout)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "    # Bottleneck\n",
    "    conv6 = conv_block(pool5, 1024, ker_init, dropout)\n",
    "    # Upsampling path\n",
    "    conv7 = up_conv_block(conv6, conv5, 512, ker_init)\n",
    "    conv8 = up_conv_block(conv7, conv4, 256, ker_init)\n",
    "    conv9 = up_conv_block(conv8, conv3, 128, ker_init)\n",
    "    conv10 = up_conv_block(conv9, conv2, 64, ker_init)\n",
    "    conv11 = up_conv_block(conv10, conv1, 32, ker_init)\n",
    "    outputs = Conv2D(4, 1, activation='softmax')(conv11)\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet('he_normal', 0.2)\n",
    "\n",
    "# Dice loss as defined above for 4 classes\n",
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    class_num = 4\n",
    "    total_loss = 0\n",
    "    for i in range(class_num):\n",
    "        y_true_f = K.flatten(y_true[:, :, :, i])\n",
    "        y_pred_f = K.flatten(y_pred[:, :, :, i])\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "        total_loss += loss\n",
    "    total_loss /= class_num\n",
    "    return total_loss\n",
    "\n",
    "# Define per class evaluation of dice coef\n",
    "def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    intersection = K.sum(K.abs(y_true[:, :, :, 1] * y_pred[:, :, :, 1]))\n",
    "    return (2. * intersection) / (K.sum(K.square(y_true[:, :, :, 1])) + K.sum(K.square(y_pred[:, :, :, 1])) + epsilon)\n",
    "\n",
    "def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    intersection = K.sum(K.abs(y_true[:, :, :, 2] * y_pred[:, :, :, 2]))\n",
    "    return (2. * intersection) / (K.sum(K.square(y_true[:, :, :, 2])) + K.sum(K.square(y_pred[:, :, :, 2])) + epsilon)\n",
    "\n",
    "def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    intersection = K.sum(K.abs(y_true[:, :, :, 3] * y_pred[:, :, :, 3]))\n",
    "    return (2. * intersection) / (K.sum(K.square(y_true[:, :, :, 3])) + K.sum(K.square(y_pred[:, :, :, 3])) + epsilon)\n",
    "\n",
    "# Computing Precision\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "# Computing Sensitivity\n",
    "def sensitivity(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "# Computing Specificity\n",
    "def specificity(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.MeanIoU(num_classes=4),\n",
    "        dice_coef,\n",
    "        precision,\n",
    "        sensitivity,\n",
    "        specificity,\n",
    "        dice_coef_necrotic,\n",
    "        dice_coef_edema,\n",
    "        dice_coef_enhancing\n",
    "    ]\n",
    ")\n",
    "# Callbacks\n",
    "csv_logger = CSVLogger('training.log', separator=',', append=False)\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='loss', min_delta=0, patience=2, verbose=1, mode='auto'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.000001, verbose=1),\n",
    "    ModelCheckpoint(filepath='model_{epoch:02d}-{val_loss:.6f}.weights.h5', verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    csv_logger\n",
    "]\n",
    "# Train the model\n",
    "history =  model.fit(train_generator,\n",
    "                    epochs=35,\n",
    "                    steps_per_epoch=len(train_ids),\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data = val_generator,\n",
    "                    verbose=1\n",
    "                    )\n",
    "model.save(\"brain_tumor.h5\")\n",
    "# # Evaluate the model on the test set\n",
    "test_metrics = model.evaluate(test_generator, verbose=1)\n",
    "print(f'Test set metrics: {test_metrics}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
