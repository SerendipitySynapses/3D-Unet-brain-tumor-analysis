{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_layer = Input((128, 128, 4))\n",
    "model = build_unet(input_layer, 'he_normal', 0.2)\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.MeanIoU(num_classes=4),\n",
    "        dice_coef,\n",
    "        precision,\n",
    "        sensitivity,\n",
    "        specificity,\n",
    "        dice_coef_necrotic,\n",
    "        dice_coef_edema,\n",
    "        dice_coef_enhancing\n",
    "    ]\n",
    ")\n",
    "plot_model(model, \n",
    "           show_shapes = True,\n",
    "           show_dtype=False,\n",
    "           show_layer_names = True, \n",
    "           rankdir = 'TB', \n",
    "           expand_nested = False, \n",
    "           dpi = 70)\n"
   ],
   "id": "3e85e5dd18a78971"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "17d862cb6ebb8844"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:38:10.013504Z",
     "start_time": "2024-06-17T17:38:09.969481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_nifti_file(filepath):\n",
    "    #Load a NIfTI file and return its data as a numpy array\n",
    "    scan = nib.load(filepath)\n",
    "    return scan.get_fdata()\n",
    "\n",
    "def normalize(volume):\n",
    "    #Normalize the volume by clipping and standardizing\n",
    "    min_val = -1000\n",
    "    max_val = 400\n",
    "    volume = np.clip(volume, min_val, max_val)\n",
    "    mean = np.mean(volume)\n",
    "    std = np.std(volume)\n",
    "    volume = (volume - mean) / std\n",
    "    return volume\n",
    "\n",
    "\n",
    "\n",
    "input_dir = './Dataset'\n",
    "output_dir = './Dataset/Processed'\n",
    "total_size={'Training':369,'Validation':125}\n",
    "\n",
    "input={'Training':['flair', 't1', 't1ce', 't2','seg'],\n",
    " 'Validation':['flair', 't1', 't1ce', 't2']}\n",
    "\n",
    "SEGMENT_CLASSES = {\n",
    "    0 : 'NOT tumor',\n",
    "    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n",
    "    2 : 'EDEMA',\n",
    "    3 : 'ENHANCING' # original 4 -> converted into 3 later\n",
    "}\n",
    "def Datagen(phase,chunk_index,chunk_size,input_dir, output_dir):\n",
    "        start_id = chunk_index * chunk_size + 1\n",
    "        end_id = min((chunk_index + 1) * chunk_size, total_size[phase])\n",
    "        for i in range(start_id,end_id):\n",
    "            id = f\"{i:03d}\"\n",
    "            npy_folder_path = os.path.join(output_dir, phase, f'chunk_{chunk_index}',id)\n",
    "            os.makedirs(npy_folder_path, exist_ok=True)\n",
    "            for modality in input[phase]:\n",
    "                    if modality != 'seg':\n",
    "                        #Modality data\n",
    "                        channel_name = f'BraTS20_{phase}_{id}_{modality}'\n",
    "                        channel_path = os.path.join(input_dir, phase, modality)\n",
    "                        channel = load_nifti_file(os.path.join(channel_path, channel_name+ '.nii'))\n",
    "                        # Save numpy file\n",
    "                        npy_channel_file_path=os.path.join(npy_folder_path,channel_name+ '.npy')\n",
    "                        np.save(npy_channel_file_path, channel)\n",
    "                    else:\n",
    "                        #Mask data\n",
    "                        mask_path= os.path.join(input_dir, phase, 'mask')\n",
    "                        mask_name= f'BraTS20_{phase}_{id}_{modality}'\n",
    "                        mask = load_nifti_file(os.path.join(mask_path, mask_name + '.nii'))\n",
    "                        # Save numpy file\n",
    "                        npy_mask_file_path = os.path.join(npy_folder_path, mask_name + '.npy')\n",
    "                        np.save(npy_mask_file_path, mask)\n",
    "        \n",
    "        # Dataset generation          \n",
    "        X = np.empty([chunk_size*155,128, 128,4])\n",
    "        y = np.empty([chunk_size*155,240, 240], dtype=int)\n",
    "        Y = np.empty([chunk_size*155,128,128, 4])\n",
    "        \n",
    "        if phase=='Training':\n",
    "            for i in range(start_id,end_id):\n",
    "                id = f\"{i:03d}\"\n",
    "                folder_path = os.path.join(output_dir, phase, f'chunk_{chunk_index}',id)\n",
    "                # Train datagen\n",
    "                for k,modality in enumerate(input[phase]):\n",
    "                    if modality!='seg':\n",
    "                        nmpy_channel= np.load(os.path.join(folder_path,f'BraTS20_{phase}_{id}_{modality}'+ '.npy'))\n",
    "                        for h in range(155):\n",
    "                            X[h , :, :, k] = cv2.resize(nmpy_channel[:, :, h], (128, 128))\n",
    "                    else:\n",
    "                        npy_mask=np.load(os.path.join(folder_path, f'BraTS20_{phase}_{id}_{modality}' + '.npy'))\n",
    "                        for h in range(155):\n",
    "                            y[h] = npy_mask[:, :, h]\n",
    "                            \n",
    "            y[y==4] = 3\n",
    "            mask = tf.one_hot(y, 4)\n",
    "            Y = tf.image.resize(mask, (128, 128))\n",
    "            return X/np.max(X), Y\n",
    "        \n",
    "            # validation datagen                \n",
    "        if phase=='Validation':\n",
    "            for i in range(start_id,end_id):\n",
    "                id = f\"{i:03d}\"\n",
    "                folder_path = os.path.join(output_dir, phase, f'chunk_{chunk_index}',id)\n",
    "                for k,modality in enumerate(input[phase]):\n",
    "                    nmpy_channel= np.load(os.path.join(folder_path,f'BraTS20_{phase}_{id}_{modality}'+ '.npy'))\n",
    "                    for h in range(155):\n",
    "                        X[h , :, :, k] = cv2.resize(nmpy_channel[:, :, h], (128, 128))\n",
    "            return X/np.max(X)\n",
    "       \n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:38:20.689342Z",
     "start_time": "2024-06-17T17:38:12.347818Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, y_train = Datagen(phase='Training', chunk_index=0, chunk_size=10, input_dir=input_dir, output_dir=output_dir)",
   "id": "1ae3301d0889c51d",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:40:51.335398Z",
     "start_time": "2024-06-17T17:40:50.593575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Y=y_train.numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, Y, test_size=0.2, random_state=42)"
   ],
   "id": "23e1ca08b731e698",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T17:30:57.094660Z",
     "start_time": "2024-06-17T17:30:50.580223Z"
    }
   },
   "cell_type": "code",
   "source": "X_val = Datagen(phase='Validation', chunk_index=0, chunk_size=10, input_dir=input_dir, output_dir=output_dir)",
   "id": "1925a2fb944a33a7",
   "outputs": [],
   "execution_count": 191
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Model Artichecture\n",
   "id": "4cf94961b75b92d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Train ",
   "id": "60ca549c22527a10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Evaluate",
   "id": "f66add214aeedee8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, Concatenate, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def conv_block(inputs, num_filters):\n",
    "    \"\"\"Convolutional block consisting of two Conv3D layers followed by an activation function.\"\"\"\n",
    "    x = Conv3D(num_filters, 3, padding='same')(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv3D(num_filters, 3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    \"\"\"Encoder block consisting of a conv block followed by a max pooling layer.\"\"\"\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters):\n",
    "    \"\"\"Decoder block consisting of an upsampling layer followed by a conv block.\"\"\"\n",
    "    x = UpSampling3D(size=(2, 2, 2))(inputs)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "def unet_model(input_shape):\n",
    "    \"\"\"Builds the 3D U-Net model.\"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 32)\n",
    "    s2, p2 = encoder_block(p1, 64)\n",
    "    s3, p3 = encoder_block(p2, 128)\n",
    "    s4, p4 = encoder_block(p3, 256)\n",
    "\n",
    "    # Bridge\n",
    "    b1 = conv_block(p4, 512)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b1, s4, 256)\n",
    "    d2 = decoder_block(d1, s3, 128)\n",
    "    d3 = decoder_block(d2, s2, 64)\n",
    "    d4 = decoder_block(d3, s1, 32)\n",
    "\n",
    "    outputs = Conv3D(1, 1, padding='same', activation='sigmoid')(d4)\n",
    "\n",
    "    model = Model(inputs, outputs, name='3d_unet')\n",
    "    return model\n",
    "model = unet_model(input_shape=(*image_size, 1))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('unet_brats2020.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=[checkpoint, early_stopping])\n",
    "model.save_weights('model_weights.h5')"
   ],
   "id": "89fd4675dae8092f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d7fc3c24837c87e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "45e42e4fd7fc9e40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "54522a59d7c15f9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2da371b94df474a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4eec6917d585c8af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6a9b88752cdfe168",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "77eafc516bc1bd5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3890a57204b123b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "70aa3044daaec4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1888d172902fc020",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Iterate over each chunk\n",
    "for start in range(1, total_files + 1, chunk_size):\n",
    "    Datagen(phase='Training', chunk_index=start, chunk_size=chunk_size, input_dir=input_dir, output_dir=output_dir)\n",
    "    end = min(start_id + chunk_size - 1, total_files)"
   ],
   "id": "80f9a17e47e011a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
