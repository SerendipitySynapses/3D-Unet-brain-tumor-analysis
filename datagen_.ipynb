{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "input_dir = './Dataset'\n",
    "output_dir = './Dataset/Processed'\n",
    "total_size={'Training':369,'Validation':125}\n",
    "modalities=['flair', 't1', 't1ce', 't2']\n",
    "\n",
    "def load_nifti_file(filepath):\n",
    "    #Load a NIfTI file and return its data as a numpy array\n",
    "    scan = nib.load(filepath)\n",
    "    return scan.get_fdata()\n",
    "\n",
    "def load_data(phase,chunk_index,chunk_size,input_dir, output_dir):\n",
    "        start_id = chunk_index * chunk_size + 1\n",
    "        end_id = min((chunk_index + 1) * chunk_size, total_size[phase])\n",
    "        for i in range(start_id,end_id+1):\n",
    "            id = f\"{i:03d}\"\n",
    "            npy_folder_path = os.path.join(output_dir, phase, f'chunk_{chunk_index}',id)\n",
    "            os.makedirs(npy_folder_path, exist_ok=True)\n",
    "            for modality in modalities:\n",
    "                #Modality data\n",
    "                channel_name = f'BraTS20_{phase}_{id}_{modality}'\n",
    "                channel_path = os.path.join(input_dir, phase, modality)\n",
    "                channel = load_nifti_file(os.path.join(channel_path, channel_name+ '.nii'))\n",
    "                # Save numpy channel file\n",
    "                npy_channel_file_path=os.path.join(npy_folder_path,channel_name+ '.npy')\n",
    "                np.save(npy_channel_file_path, channel)\n",
    "\n",
    "            #Mask data\n",
    "            mask_path= os.path.join(input_dir, phase, 'mask')\n",
    "            mask_name= f'BraTS20_{phase}_{id}_{'seg'}'\n",
    "            mask = load_nifti_file(os.path.join(mask_path, mask_name + '.nii'))\n",
    "            # Save numpy mask file\n",
    "            npy_mask_file_path = os.path.join(npy_folder_path, mask_name + '.npy')\n",
    "            np.save(npy_mask_file_path, mask)\n",
    "            \n",
    "X = np.empty((0, 128, 128, 4))\n",
    "y = np.empty((0, 128, 128, 4), dtype=int)\n",
    "for index in range(9):\n",
    "    chunk_size=41\n",
    "    phase='Training'\n",
    "    load_data(phase=phase,chunk_index=index,chunk_size=chunk_size,input_dir=input_dir, output_dir=output_dir)\n",
    "    chunk_index=index\n",
    "    start_id = chunk_index * chunk_size + 1\n",
    "    end_id = min((chunk_index + 1) * chunk_size, total_size[phase])\n",
    "    x = np.empty([chunk_size * 155, 128, 128, 4])\n",
    "    y_tr = np.empty([chunk_size*155,128, 128,4], dtype=int)\n",
    "    # training\n",
    "    for i in range(start_id,end_id+1):\n",
    "        id = f\"{i:03d}\"\n",
    "        numpy_folder_path = os.path.join(output_dir, phase, f'chunk_{chunk_index}',id)\n",
    "        for h in range(155):\n",
    "            # y\n",
    "            npy_mask=np.load(os.path.join(numpy_folder_path, f'BraTS20_{phase}_{id}_{'seg'}' + '.npy'))\n",
    "            mask = tf.one_hot(npy_mask[:, :, h], 4)  # One-hot encoding\n",
    "            Y = tf.image.resize(mask, (128, 128))  # Resize\n",
    "            y_tr[(i - start_id) * 155 + h] = Y.numpy()\n",
    "            # X_train\n",
    "            for k,modality in enumerate(modalities):\n",
    "                nmpy_channel= np.load(os.path.join(numpy_folder_path,f'BraTS20_{phase}_{id}_{modality}'+ '.npy'))\n",
    "                x[(i - start_id) * 155 + h , :, :, k] = cv2.resize(nmpy_channel[:, :, h], (128, 128))\n",
    "\n",
    "    y_tr[y_tr==4] = 3\n",
    "    \n",
    "    # Concatenate the current chunk with the existing data\n",
    "    X = np.concatenate((X, x), axis=0)\n",
    "    y = np.concatenate((y, y_tr), axis=0)\n",
    "    shutil.rmtree(os.path.join(output_dir, phase, f'chunk_{chunk_index}'))\n",
    "\n",
    "\n",
    "print(f'Final X shape={X.shape}, y shape={y.shape}')"
   ],
   "id": "8c1f4bfea93f2c44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "class NiftiProcessor:\n",
    "    def __init__(self, input_dir, output_dir, total_size, modalities):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.total_size = total_size\n",
    "        self.modalities = modalities\n",
    "        self.X = np.empty((0, 128, 128, 4))\n",
    "        self.y = np.empty((0, 128, 128, 4), dtype=int)\n",
    "\n",
    "    def load_nifti_file(self, filepath):\n",
    "        # Load a NIfTI file and return its data as a numpy array\n",
    "        scan = nib.load(filepath)\n",
    "        return scan.get_fdata()\n",
    "\n",
    "    def load_data(self, phase, chunk_index, chunk_size):\n",
    "        start_id = chunk_index * chunk_size + 1\n",
    "        end_id = min((chunk_index + 1) * chunk_size, self.total_size[phase])\n",
    "        for i in range(start_id, end_id + 1):\n",
    "            id = f\"{i:03d}\"\n",
    "            npy_folder_path = os.path.join(self.output_dir, phase, f'chunk_{chunk_index}', id)\n",
    "            os.makedirs(npy_folder_path, exist_ok=True)\n",
    "            for modality in self.modalities:\n",
    "                # Modality data\n",
    "                channel_name = f'BraTS20_{phase}_{id}_{modality}'\n",
    "                channel_path = os.path.join(self.input_dir, phase, modality)\n",
    "                channel = self.load_nifti_file(os.path.join(channel_path, channel_name + '.nii'))\n",
    "                # Save numpy channel file\n",
    "                npy_channel_file_path = os.path.join(npy_folder_path, channel_name + '.npy')\n",
    "                np.save(npy_channel_file_path, channel)\n",
    "\n",
    "            # Mask data\n",
    "            mask_path = os.path.join(self.input_dir, phase, 'mask')\n",
    "            mask_name = f'BraTS20_{phase}_{id}_seg'\n",
    "            mask = self.load_nifti_file(os.path.join(mask_path, mask_name + '.nii'))\n",
    "            # Save numpy mask file\n",
    "            npy_mask_file_path = os.path.join(npy_folder_path, mask_name + '.npy')\n",
    "            np.save(npy_mask_file_path, mask)\n",
    "\n",
    "    def process_data(self, phase, chunk_index, chunk_size):\n",
    "        start_id = chunk_index * chunk_size + 1\n",
    "        end_id = min((chunk_index + 1) * chunk_size, self.total_size[phase])\n",
    "        x = np.empty([chunk_size * 155, 128, 128, 4])\n",
    "        y_tr = np.empty([chunk_size * 155, 128, 128, 4], dtype=int)\n",
    "        for i in range(start_id, end_id + 1):\n",
    "            id = f\"{i:03d}\"\n",
    "            numpy_folder_path = os.path.join(self.output_dir, phase, f'chunk_{chunk_index}', id)\n",
    "            for h in range(155):\n",
    "                # y\n",
    "                npy_mask = np.load(os.path.join(numpy_folder_path, f'BraTS20_{phase}_{id}_seg.npy'))\n",
    "                mask = tf.one_hot(npy_mask[:, :, h], 4)  # One-hot encoding\n",
    "                Y = tf.image.resize(mask, (128, 128))  # Resize\n",
    "                y_tr[(i - start_id) * 155 + h] = Y.numpy()\n",
    "                # X_train\n",
    "                for k, modality in enumerate(self.modalities):\n",
    "                    nmpy_channel = np.load(os.path.join(numpy_folder_path, f'BraTS20_{phase}_{id}_{modality}.npy'))\n",
    "                    x[(i - start_id) * 155 + h, :, :, k] = cv2.resize(nmpy_channel[:, :, h], (128, 128))\n",
    "                print((i - start_id) * 155 + h)\n",
    "        y_tr[y_tr == 4] = 3\n",
    "        self.X = np.concatenate((self.X, x), axis=0)\n",
    "        self.y = np.concatenate((self.y, y_tr), axis=0)\n",
    "        shutil.rmtree(os.path.join(self.output_dir, phase, f'chunk_{chunk_index}'))\n",
    "\n",
    "    def run(self, phase, num_chunks, chunk_size):\n",
    "        for index in range(num_chunks):\n",
    "            print(index)\n",
    "            self.load_data(phase=phase, chunk_index=index, chunk_size=chunk_size)\n",
    "            self.process_data(phase=phase, chunk_index=index, chunk_size=chunk_size)\n",
    "        print(f'Final X shape={self.X.shape}, y shape={self.y.shape}')\n",
    "\n",
    "\n",
    "input_dir = './Dataset'\n",
    "output_dir = './Dataset/Processed'\n",
    "total_size = {'Training': 369, 'Validation': 125}\n",
    "modalities = ['flair', 't1', 't1ce', 't2']\n",
    "num_chunks = 3\n",
    "chunk_size = 41\n",
    "\n",
    "processor = NiftiProcessor(input_dir, output_dir, total_size, modalities)\n",
    "processor.run(phase='Training', num_chunks=num_chunks, chunk_size=chunk_size)"
   ],
   "id": "23b30fe1fe2d2543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,shuffle=True,test_size=0.2)"
   ],
   "id": "20a279286be58904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Unsupervised validation set\n",
    "X_validation= np.empty([0,128, 128,4])\n",
    "    for chunk_index in range(2):\n",
    "        chunk_size=10\n",
    "        phase='Validation'\n",
    "        start_id = chunk_index * chunk_size + 1\n",
    "        end_id = min((chunk_index + 1) * chunk_size, total_size[phase])\n",
    "        X_val = np.empty([chunk_size*155,128, 128,4])\n",
    "        # validation\n",
    "        for i in range(start_id,end_id+1):\n",
    "            id = f\"{i:03d}\"\n",
    "            numpy_folder_path = os.path.join(output_dir, phase, f'chunk_{chunk_index}',id)\n",
    "            for h in range(155):\n",
    "                # X_val\n",
    "                for k,modality in enumerate(modalities):\n",
    "                    nmpy_channel= np.load(os.path.join(numpy_folder_path,f'BraTS20_{phase}_{id}_{modality}'+ '.npy'))\n",
    "                    X_val[(i - start_id) * 155 + h , :, :, k] = cv2.resize(nmpy_channel[:, :, h], (128, 128))        \n",
    "        # Concatenate the current chunk with the existing data\n",
    "        X_validation = np.concatenate((X, X_train), axis=0)"
   ],
   "id": "7a1eddc0f0bb9c89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# def train_datagen(phase,chunk_index,chunk_size,X_train,y_train):\n",
    "#         start_id = chunk_index * chunk_size + 1\n",
    "#         end_id = min((chunk_index + 1) * chunk_size, total_size[phase])\n",
    "#         # training\n",
    "#         for i in range(start_id,end_id):\n",
    "#             id = f\"{i:03d}\"\n",
    "#             numpy_folder_path = os.path.join(output_dir, phase, f'chunk_{chunk_index}',id)\n",
    "#             for h in range(155):\n",
    "#                 # y\n",
    "#                 npy_mask=np.load(os.path.join(numpy_folder_path, f'BraTS20_{phase}_{id}_{'seg'}' + '.npy'))\n",
    "#                 y_train[h+155*chunk_index] = npy_mask[:, :, h]\n",
    "#                 # X_train\n",
    "#                 for k,modality in enumerate(modalities):\n",
    "#                     nmpy_channel= np.load(os.path.join(numpy_folder_path,f'BraTS20_{phase}_{id}_{modality}'+ '.npy'))\n",
    "#                     X_train[h+155*chunk_index , :, :, k] = cv2.resize(nmpy_channel[:, :, h], (128, 128))\n",
    "# \n",
    "#         y_train[y_train==4] = 3\n",
    "#         # mask = tf.one_hot(y_train, 4)\n",
    "#         # Y_train = tf.image.resize(mask, (128, 128))\n",
    "#         return X_train/np.max(X_train), Y_train"
   ],
   "id": "413c9cf05cd5de6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6478ebad537c6752",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "facd72f20e5f195c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6d7eeacff1aa3ded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Model Artichecture\n",
   "id": "4cf94961b75b92d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SEGMENT_CLASSES = {\n",
    "    0 : 'NOT tumor',\n",
    "    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n",
    "    2 : 'EDEMA',\n",
    "    3 : 'ENHANCING' # original 4 -> converted into 3 later\n",
    "}\n",
    "input_layer = Input((128, 128, 4))\n",
    "model = build_unet(input_layer, 'he_normal', 0.2)\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.MeanIoU(num_classes=4),\n",
    "        dice_coef,\n",
    "        precision,\n",
    "        sensitivity,\n",
    "        specificity,\n",
    "        dice_coef_necrotic,\n",
    "        dice_coef_edema,\n",
    "        dice_coef_enhancing\n",
    "    ]\n",
    ")\n",
    "plot_model(model, \n",
    "           show_shapes = True,\n",
    "           show_dtype=False,\n",
    "           show_layer_names = True, \n",
    "           rankdir = 'TB', \n",
    "           expand_nested = False, \n",
    "           dpi = 70)\n"
   ],
   "id": "3e85e5dd18a78971",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Train ",
   "id": "60ca549c22527a10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Evaluate",
   "id": "f66add214aeedee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, Concatenate, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def conv_block(inputs, num_filters):\n",
    "    \"\"\"Convolutional block consisting of two Conv3D layers followed by an activation function.\"\"\"\n",
    "    x = Conv3D(num_filters, 3, padding='same')(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv3D(num_filters, 3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    \"\"\"Encoder block consisting of a conv block followed by a max pooling layer.\"\"\"\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters):\n",
    "    \"\"\"Decoder block consisting of an upsampling layer followed by a conv block.\"\"\"\n",
    "    x = UpSampling3D(size=(2, 2, 2))(inputs)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "def unet_model(input_shape):\n",
    "    \"\"\"Builds the 3D U-Net model.\"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 32)\n",
    "    s2, p2 = encoder_block(p1, 64)\n",
    "    s3, p3 = encoder_block(p2, 128)\n",
    "    s4, p4 = encoder_block(p3, 256)\n",
    "\n",
    "    # Bridge\n",
    "    b1 = conv_block(p4, 512)\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b1, s4, 256)\n",
    "    d2 = decoder_block(d1, s3, 128)\n",
    "    d3 = decoder_block(d2, s2, 64)\n",
    "    d4 = decoder_block(d3, s1, 32)\n",
    "\n",
    "    outputs = Conv3D(1, 1, padding='same', activation='sigmoid')(d4)\n",
    "\n",
    "    model = Model(inputs, outputs, name='3d_unet')\n",
    "    return model\n",
    "model = unet_model(input_shape=(*image_size, 1))\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('unet_brats2020.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=[checkpoint, early_stopping])\n",
    "model.save_weights('model_weights.h5')"
   ],
   "id": "89fd4675dae8092f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d7fc3c24837c87e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "45e42e4fd7fc9e40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "54522a59d7c15f9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2da371b94df474a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4eec6917d585c8af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6a9b88752cdfe168",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "77eafc516bc1bd5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3890a57204b123b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "70aa3044daaec4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1888d172902fc020",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate over each chunk\n",
    "for start in range(1, total_files + 1, chunk_size):\n",
    "    Datagen(phase='Training', chunk_index=start, chunk_size=chunk_size, input_dir=input_dir, output_dir=output_dir)\n",
    "    end = min(start_id + chunk_size - 1, total_files)"
   ],
   "id": "80f9a17e47e011a7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
