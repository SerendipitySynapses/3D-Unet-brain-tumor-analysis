{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "21e341f399b82cb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, list_IDs, input_dir, batch_size=2, dim=(128, 128, 128), n_channels=4, n_classes=4, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.input_dir = input_dir\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __load_nifti_file(self, filepath):\n",
    "        scan = nib.load(filepath)\n",
    "        return scan.get_fdata()\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            folder_name = os.path.join(self.input_dir, f'BraTS20_Training_{ID}')\n",
    "            for modality_index, modality in enumerate(['flair', 't1', 't1ce', 't2']):\n",
    "                modality_path = os.path.join(folder_name, f'BraTS20_Training_{ID}_{modality}.nii')\n",
    "                modality_data = self.__load_nifti_file(modality_path)\n",
    "                resized_data = cv2.resize(modality_data, (self.dim[0], self.dim[1]))\n",
    "                X[i, :, :, :, modality_index] = resized_data\n",
    "            \n",
    "            mask_path = os.path.join(folder_name, f'BraTS20_Training_{ID}_seg.nii')\n",
    "            mask_data = self.__load_nifti_file(mask_path)\n",
    "            mask_resized = tf.image.resize(mask_data, (self.dim[0], self.dim[1]))\n",
    "            y[i] = tf.one_hot(mask_resized, self.n_classes)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "# Define Dice coefficient metric\n",
    "def dice_coefficient(y_true, y_pred, smooth=1):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "# U-Net model definition\n",
    "def unet_model(input_size=(128, 128, 128, 4), n_classes=4):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling3D(pool_size=(2, 2, 2))(drop4)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv5 = Conv3D(1024, (3, 3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv3D(1024, (3, 3, 3), activation='relu', padding='same')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    # Decoder\n",
    "    up6 = UpSampling3D(size=(2, 2, 2))(drop5)\n",
    "    up6 = Conv3D(512, (2, 2, 2), activation='relu', padding='same')(up6)\n",
    "    merge6 = concatenate([drop4, up6], axis=4)\n",
    "    conv6 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(merge6)\n",
    "    conv6 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = UpSampling3D(size=(2, 2, 2))(conv6)\n",
    "    up7 = Conv3D(256, (2, 2, 2), activation='relu', padding='same')(up7)\n",
    "    merge7 = concatenate([conv3, up7], axis=4)\n",
    "    conv7 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(merge7)\n",
    "    conv7 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = UpSampling3D(size=(2, 2, 2))(conv7)\n",
    "    up8 = Conv3D(128, (2, 2, 2), activation='relu', padding='same')(up8)\n",
    "    merge8 = concatenate([conv2, up8], axis=4)\n",
    "    conv8 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(merge8)\n",
    "    conv8 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = UpSampling3D(size=(2, 2, 2))(conv8)\n",
    "    up9 = Conv3D(64, (2, 2, 2), activation='relu', padding='same')(up9)\n",
    "    merge9 = concatenate([conv1, up9], axis=4)\n",
    "    conv9 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(merge9)\n",
    "    conv9 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = Conv3D(2, (3, 3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv10 = Conv3D(n_classes, (1, 1, 1), activation='softmax')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare IDs and data generators\n",
    "input_dir = './Dataset/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
    "train_and_test_ids = [f.name.split('_')[-1] for f in os.scandir(input_dir) if f.is_dir()]\n",
    "train_and_test_ids, val_ids = train_test_split(train_and_test_ids, test_size=0.2)\n",
    "train_ids, test_ids = train_test_split(train_and_test_ids, test_size=0.15, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(train_ids, input_dir)\n",
    "val_generator = DataGenerator(val_ids, input_dir)\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = unet_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[dice_coefficient])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=50)\n"
   ],
   "id": "c986fbedf9dd70fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, list_IDs, input_dir, batch_size=2, dim=(128, 128, 128), n_channels=4, n_classes=4, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.input_dir = input_dir\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __load_nifti_file(self, filepath):\n",
    "        scan = nib.load(filepath)\n",
    "        return scan.get_fdata()\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            folder_name = os.path.join(self.input_dir, f'BraTS20_Training_{ID}')\n",
    "            for modality_index, modality in enumerate(['flair', 't1', 't1ce', 't2']):\n",
    "                modality_path = os.path.join(folder_name, f'BraTS20_Training_{ID}_{modality}.nii')\n",
    "                modality_data = self.__load_nifti_file(modality_path)\n",
    "                resized_data = cv2.resize(modality_data, (self.dim[0], self.dim[1]))\n",
    "                X[i, :, :, :, modality_index] = resized_data\n",
    "            \n",
    "            mask_path = os.path.join(folder_name, f'BraTS20_Training_{ID}_seg.nii')\n",
    "            mask_data = self.__load_nifti_file(mask_path)\n",
    "            mask_resized = tf.image.resize(mask_data, (self.dim[0], self.dim[1]))\n",
    "            y[i] = tf.one_hot(mask_resized, self.n_classes)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "# Define Dice coefficient metric\n",
    "def dice_coefficient(y_true, y_pred, smooth=1):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "# U-Net model definition\n",
    "def unet_model(input_size=(128, 128, 128, 4), n_classes=4):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling3D(pool_size=(2, 2, 2))(drop4)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv5 = Conv3D(1024, (3, 3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv3D(1024, (3, 3, 3), activation='relu', padding='same')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    # Decoder\n",
    "    up6 = UpSampling3D(size=(2, 2, 2))(drop5)\n",
    "    up6 = Conv3D(512, (2, 2, 2), activation='relu', padding='same')(up6)\n",
    "    merge6 = concatenate([drop4, up6], axis=4)\n",
    "    conv6 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(merge6)\n",
    "    conv6 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = UpSampling3D(size=(2, 2, 2))(conv6)\n",
    "    up7 = Conv3D(256, (2, 2, 2), activation='relu', padding='same')(up7)\n",
    "    merge7 = concatenate([conv3, up7], axis=4)\n",
    "    conv7 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(merge7)\n",
    "    conv7 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = UpSampling3D(size=(2, 2, 2))(conv7)\n",
    "    up8 = Conv3D(128, (2, 2, 2), activation='relu', padding='same')(up8)\n",
    "    merge8 = concatenate([conv2, up8], axis=4)\n",
    "    conv8 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(merge8)\n",
    "    conv8 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = UpSampling3D(size=(2, 2, 2))(conv8)\n",
    "    up9 = Conv3D(64, (2, 2, 2), activation='relu', padding='same')(up9)\n",
    "    merge9 = concatenate([conv1, up9], axis=4)\n",
    "    conv9 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(merge9)\n",
    "    conv9 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = Conv3D(2, (3, 3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv10 = Conv3D(n_classes, (1, 1, 1), activation='softmax')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare IDs and data generators\n",
    "input_dir = './Dataset/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
    "train_and_test_ids = [f.name.split('_')[-1] for f in os.scandir(input_dir) if f.is_dir()]\n",
    "train_and_test_ids, val_ids = train_test_split(train_and_test_ids, test_size=0.2)\n",
    "train_ids, test_ids = train_test_split(train_and_test_ids, test_size=0.15, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(train_ids, input_dir)\n",
    "val_generator = DataGenerator(val_ids, input_dir)\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = unet_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[dice_coefficient])\n",
    "\n",
    "# Custom training loop with progress bar\n",
    "epochs = 50\n",
    "steps_per_epoch = len(train_generator)\n",
    "validation_steps = len(val_generator)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    with tqdm(total=steps_per_epoch, desc='Training', unit='batch') as pbar:\n",
    "        for step in range(steps_per_epoch):\n",
    "            X_batch, y_batch = train_generator[step]\n",
    "            loss = model.train_on_batch(X_batch, y_batch)\n",
    "            pbar.set_postfix({'loss': loss[0], 'dice_coefficient': loss[1]})\n",
    "            pbar.update(1)\n",
    "\n",
    "    with tqdm(total=validation_steps, desc='Validation', unit='batch') as pbar:\n",
    "        val_loss = []\n",
    "        for step in range(validation_steps):\n",
    "            X_val_batch, y_val_batch = val_generator[step]\n",
    "            loss = model.test_on_batch(X_val_batch, y_val_batch)\n",
    "            val_loss.append(loss)\n",
    "            pbar.set_postfix({'val_loss': loss[0], 'val_dice_coefficient': loss[1]})\n",
    "            pbar.update(1)\n",
    "\n",
    "    avg_val_loss = np.mean(val_loss, axis=0)\n",
    "    print(f'Validation loss: {avg_val_loss[0]}, Validation Dice Coefficient: {avg_val_loss[1]}')\n",
    "\n"
   ],
   "id": "c7dfa8d73915ae14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NiftiDataGenerator(Sequence):\n",
    "    def __init__(self, input_dir, total_size, modalities, batch_size=32, dim=(128, 128), n_channels=4, n_classes=4, shuffle=True):\n",
    "        self.input_dir = input_dir\n",
    "        self.total_size = total_size\n",
    "        self.modalities = modalities\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.total_size['Training'] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_ids = [k for k in indexes]\n",
    "        X, y = self.__data_generation(batch_ids)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(self.total_size['Training'])\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        X = np.empty((self.batch_size * 155, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size * 155, *self.dim, self.n_classes), dtype=int)\n",
    "\n",
    "        with tqdm(total=self.batch_size * 155, desc='Generating batch data', unit='slice') as pbar:\n",
    "            for i, ID in enumerate(batch_ids):\n",
    "                id_str = f\"{ID + 1:03d}\"\n",
    "                for h in range(155):\n",
    "                    for k, modality in enumerate(self.modalities):\n",
    "                        channel_name = f'BraTS20_Training_{id_str}_{modality}'\n",
    "                        channel_path = os.path.join(self.input_dir, 'Training', modality)\n",
    "                        nmpy_channel = self.load_nifti_file(os.path.join(channel_path, channel_name + '.nii'))\n",
    "                        X[i * 155 + h, :, :, k] = cv2.resize(nmpy_channel[:, :, h], self.dim)\n",
    "                    \n",
    "                    mask_name = f'BraTS20_Training_{id_str}_seg'\n",
    "                    mask_path = os.path.join(self.input_dir, 'Training', 'mask')\n",
    "                    npy_mask = self.load_nifti_file(os.path.join(mask_path, mask_name + '.nii'))\n",
    "                    mask = tf.one_hot(npy_mask[:, :, h], self.n_classes)\n",
    "                    y[i * 155 + h] = tf.image.resize(mask, self.dim).numpy()\n",
    "\n",
    "                    pbar.update(1)\n",
    "\n",
    "        y[y == self.n_classes] = self.n_classes - 1\n",
    "        return X, y\n",
    "\n",
    "    def load_nifti_file(self, filepath):\n",
    "        scan = nib.load(filepath)\n",
    "        return scan.get_fdata()\n",
    "\n",
    "input_dir = './Dataset'\n",
    "total_size = {'Training': 369, 'Validation': 125}\n",
    "modalities = ['flair', 't1', 't1ce', 't2']\n",
    "batch_size = 2\n",
    "\n",
    "training_generator = NiftiDataGenerator(input_dir=input_dir, total_size=total_size, modalities=modalities, batch_size=batch_size)\n",
    "\n",
    "# Define a model suitable for segmentation\n",
    "inputs = tf.keras.layers.Input(shape=(128, 128, 4))\n",
    "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p3)\n",
    "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c4)\n",
    "p4 = tf.keras.layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p4)\n",
    "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "u6 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u6)\n",
    "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "u7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u7)\n",
    "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "u8 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u8)\n",
    "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "u9 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c1])\n",
    "c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u9)\n",
    "c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "outputs = tf.keras.layers.Conv2D(4, (1, 1), activation='softmax')(c9)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_generator, epochs=10)\n"
   ],
   "id": "5e354c37e2d9c98c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "794841c07889b7e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5ffb862465a2fee5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
